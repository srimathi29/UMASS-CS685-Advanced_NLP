# -*- coding: utf-8 -*-
"""CS685_HW0_S24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PqDQuFX8SXGwNVO4t-tbQRDUJocV_Z2W

## Homework 0, CS685 Spring 2024

### This is due on **February 16th, 2024**, submitted via Gradescope as a PDF (File>Print>Save as PDF). 100 points total.

### IMPORTANT: After copying this notebook to your Google Drive, please paste a link to it below. To get a publicly-accessible link, hit the *Share* button at the top right, then click "Get shareable link" and copy over the result. If you fail to do this, you will receive no credit for this homework!
# ***LINK: https://colab.research.google.com/drive/1PqDQuFX8SXGwNVO4t-tbQRDUJocV_Z2W?usp=sharing***

---

##### *How to do this problem set:*

- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.

- For text-based answers, you should replace the text that says "Write your answer here..." with your actual answer.

- This assignment is designed so that you can run all cells almost instantly. If it is taking longer than that, you have made a mistake in your code.

- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!

---

##### *How to submit this problem set:*
- Write all the answers in this Colab notebook. Once you are finished, generate a PDF via (File -> Print -> Save as PDF) and upload it to Gradescope.
  
- **Important:** check your PDF before you submit to Gradescope to make sure it exported correctly. If Colab gets confused about your syntax, it will sometimes terminate the PDF creation routine early.

- **Important:** on Gradescope, please make sure that you tag each page with the corresponding question(s). This makes it significantly easier for our graders to grade submissions. We may take off points for submissions that are not tagged.

- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One handy way to do this is by clicking `Runtime -> Run All` in the notebook menu.

---

##### *Academic honesty*

- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.

- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.

---

##Question 1.1 (10 points)
Let's begin with a quick probability review. In the task of language modeling, we're interested in computing the **joint** probability of some text. Say we have a sentence $s$ with $n$ words ($w_1, w_2, w_3, \dots, w_n$) and we want to compute the joint probability $P(w_1, w_2, w_3, \dots, w_n$). Assume we are given a model that produces the conditional probability of the next word in a sentence given all preceding words: $P(w_i|w_1,w_2,\dots,w_{i-1})$. How can we use this model to compute the joint probability of sentence $s$?

---

We can use this model to compute the joint probability of a sentence s by using the chain rule.

That is:

$P(w_1, w_2, w_3, \dots, w_n$) = $P(w_1$) x $P(w_2|w_1$) x $P(w_3|w_1, w_2$) x...x $P(w_n|w_1,w_2,\dots,w_{n-1})$

##Question 1.2 (10 points)
Why would we ever want to compute the joint probability of a sentence? Provide **two** different reasons why this probability might be useful to solve an NLP task.

---

Computing the joint probability of a sentence is useful in machine translation and speech recognition. In machine translation, joint probability helps in selecting translation which leads to maximum accuracy and in speech recognition, it helps in finding the most likey sequence of words.

##Question 1.3 (5 points)
Here is a simple way to build a language model: for any prefix $w_1, w_2, \dots, w_{i-1}$, retrieve all occurrences of that prefix in some huge text corpus (such as the [Common Crawl](https://commoncrawl.org/)) and keep count of the word $w_i$ that follows each occurrence. I can then use this to estimate the conditional probability $P(w_i|w_1, w_2, \dots, w_{i-1})$ for any prefix. Explain why this method is completely impractical!

---

This method would not be practical since processing such a huge text corpus would require a lot of computational resources, storage, cost and time. There would also be data sparsity issues leading to unrealiable estimates.

##Question 2.1 (5 points)
Let's switch over to coding! The below coding cell contains the opening paragraph of Daphne du Maurier's novel *Rebecca*. Write some code in this cell to compute the number of unique word **types** and total word **tokens** in this paragraph (watch the lecture videos if you're confused about what these terms mean!). Use a whitespace tokenizer to separate words (i.e., split the string on white space using Python's split function). Be sure that the cell's output is visible in the PDF file you turn in on Gradescope.

---
"""

paragraph = '''Last night I dreamed I went to Manderley again. It seemed to me
that I was passing through the iron gates that led to the driveway.
The drive was just a narrow track now, its stony surface covered
with grass and weeds. Sometimes, when I thought I had lost it, it
would appear again, beneath a fallen tree or beyond a muddy pool
formed by the winter rains. The trees had thrown out new
low branches which stretched across my way. I came to the house
suddenly, and stood there with my heart beating fast and tears
filling my eyes.'''.lower() # lowercase normalization is often useful in NLP

types = 0
tokens = 0

# YOUR CODE HERE! POPULATE THE types AND tokens VARIABLES WITH THE CORRECT VALUES!
words = paragraph.split()  #split the paragraph into words through white space
types = len(set(words))  #count all unique word types
tokens = len(words)  #count all word tokens

# DO NOT MODIFY THE BELOW LINE!
print('Number of word types: %d, number of word tokens:%d' % (types, tokens))

"""##Question 2.2 (5 points)
Now let's look at the most frequently used word **types** in this paragraph. Write some code in the below cell to print out the ten most frequently-occurring types. We have initialized a [Counter](https://docs.python.org/2/library/collections.html#collections.Counter) object that you should use for this purpose. In general, Counters are very useful for text processing in Python.

---

"""

from collections import Counter
import re

c = Counter()
paragraph = re.sub(r'[^\w\s]', '', paragraph) #remove punctuations
words = paragraph.split()  #split the paragraph into words through white space
c = Counter(words)

# DO NOT MODIFY THE BELOW LINES!
for word, count in c.most_common()[:10]:
    print(word, count)

"""##Question 2.3 (5 points)
What do you notice about these words and their linguistic functions (i.e., parts-of-speech)? These words are known as "stopwords" in NLP and are often removed from the text before any computational modeling is done. Why do you think that is?

---

It can be obsereved that these "stopwords" occur more frequently and does not contribute individual meaning but rather they are used for grammar. They are probably removed to reduce data size and inturn focus on meaningful words and increase model efficiency.

##Question 3.1 (10 points)
In *neural* language models, we represent words with low-dimensional vectors also called *embeddings*. We use these embeddings to compute a vector representation $\boldsymbol{x}$ of a given prefix, and then predict the probability of the next word conditioned on $\boldsymbol{x}$. In the below cell, we use [PyTorch](https://pytorch.org), a machine learning framework, to explore this setup. We provide embeddings for the prefix "Alice talked to"; your job is to combine them into a single vector representation $\boldsymbol{x}$ using [element-wise vector addition](https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations).

*TIP: if you're finding the PyTorch coding problems difficult, you may want to run through [the 60 minutes blitz tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)!*

---
"""

import torch
torch.set_printoptions(sci_mode=False)
torch.manual_seed(0)

prefix = 'Alice talked to'

# spend some time understanding this code / reading relevant documentation!
# this is a toy problem with a 5 word vocabulary and 10-d embeddings
embeddings = torch.nn.Embedding(num_embeddings=5, embedding_dim=10)
vocab = {'Alice':0, 'talked':1, 'to':2, 'Bob':3, '.':4}

# we need to encode our prefix as integer indices (not words) that index
# into the embeddings matrix. the below line accomplishes this.
# note that PyTorch inputs are always Tensor objects, so we need
# to create a LongTensor out of our list of indices first.
indices = torch.LongTensor([vocab[w] for w in prefix.split()])
prefix_embs = embeddings(indices)
print('prefix embedding tensor size: ', prefix_embs.size())

# okay! we now have three embeddings corresponding to each of the three
# words in the prefix. write some code that adds them element-wise to obtain
# a representation of the prefix! store your answer in a variable named "x".

### YOUR CODE HERE!
x = prefix_embs.sum(dim=0)

### DO NOT MODIFY THE BELOW LINE
print('embedding sum: ', x)

"""##Question 3.2 (5 points)
Modern language models do not use element-wise addition to combine the different word embeddings in the prefix into a single representation (a process called *composition*). What is a major issue with element-wise functions that makes them unsuitable for use as composition functions?

---

Element wise functions consider each element of the vector to be independent and don't take into account the correlation and dependencies between them.This is the major issue with element-wise functions as the meaning of most words or phrases are subjective to it's counter parts in the embedding space.

##Question 3.3 (10 points)
One very important function in neural language models (and for basically every task we'll look at this semester) is the [softmax](https://pytorch.org/docs/master/nn.functional.html#softmax), which is defined over an $n$-dimensional vector $<x_1, x_2, \dots, x_n>$ as $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{1 \leq j \leq n} e^{x_j}}$. Let's say we have our prefix representation $\boldsymbol{x}$ from before. We can use the softmax function, along with a linear projection using a matrix $W$, to go from $\boldsymbol{x}$ to a probability distribution $p$ over the next word: $p = \text{softmax}(W\boldsymbol{x})$. Let's explore this in the code cell below:
"""

# remember, our goal is to produce a probability distribution over the
# next word, conditioned on the prefix representation x. This distribution
# is thus over the entire vocabulary (i.e., it is a 5-dimensional vector).
# take a look at the dimensionality of x, and you'll notice that it is a
# 10-dimensional vector. first, we need to **project** this representation
# down to 5-d. We'll do this using the below matrix:
W = torch.rand(10, 5)

# use this matrix to project x to a 5-d space, and then
# use the softmax function to convert it to a probability distribution.
# this will involve using PyTorch to compute a matrix/vector product.
# look through the documentation if you're confused (torch.nn.functional.softmax)
# please store your final probability distribution in the "probs" variable.

### YOUR CODE HERE
x_projected = torch.matmul(x, W)
probs = torch.nn.functional.softmax(x_projected, dim=0)


### DO NOT MODIFY THE BELOW LINE!
print('probability distribution', probs)

"""##Question 3.4 (15 points)
So far, we have looked at just a single prefix ("Alice talked to"). In practice, it is common for us to compute many prefixes in one computation, as this enables us to take advantage of GPU parallelism and also obtain better gradient approximations (we'll talk more about the latter point later). This is called *batching*, where each prefix is an example in a larger batch. Here, you'll redo the computations from the previous cells, but instead of having one prefix, you'll have a batch of two prefixes. The final output of this cell should be a 2x5 matrix that contains two probability distributions, one for each prefix. **NOTE: YOU WILL LOSE POINTS IF YOU USE ANY LOOPS IN YOUR ANSWER!** Your code should be completely vectorized (a few large computations is faster than many smaller ones).
"""

# for this problem, we'll just copy our old prefix over three times
# to form a batch. in practice, each example in the batch would be different.
batch_indices = torch.cat(2 * [indices]).reshape((2, 3))
batch_embs = embeddings(batch_indices)
print('batch embedding tensor size: ', batch_embs.size())

# now, follow the same procedure as before:
# step 1: compose each example's embeddings into a single representation
# using element-wise addition. HINT: check out the "dim" argument of the torch.sum function!
batch_x = torch.sum(batch_embs, dim=1)

# step 2: project each composed representation into a 5-d space using matrix W
projected_x = torch.matmul(batch_x, W)

# step 3: use the softmax function to obtain a 2x5 matrix with the probability distributions
# please store this probability matrix in the "batch_probs" variable, which is
# currently initialized with random numbers.
#batch_probs = torch.rand(2,5)
batch_probs = torch.nn.functional.softmax(projected_x, dim=1)

# please store this probability matrix in the "batch_probs" variable, which is
# currently initialized with random numbers.
#batch_probs = torch.rand(2,5)


### DO NOT MODIFY THE BELOW LINE
print("batch probability distributions:", batch_probs)

"""## Question 4 (20 points)

Choose  one  paper  from [ICLR 2024](https://openreview.net/group?id=ICLR.cc/2024/Conference#tab-accept-oral) that is (1) related to language modeling and (2) of interest to you. It doesn't matter if the paper was accepted or rejected. A good way to do this is by searching for some keywords and then scanning the resulting titles and abstracts; there are thousands of papers so take your time before selecting one!  Then, write a summary in your own words of the paper you chose. Your summary should answer the following questions: what is its motivation? Why should anyone care about it? Were there things in the paper that you didn't understand at all? What were they? Fill out the below cell, and make sure to write 2-4 paragraphs for the summary to receive full credit!

**Title of paper**: LLMCARBON: MODELING THE END-TO-END CARBON FOOTPRINT OF LARGE LANGUAGE MODELS

**Authors**: Anonymous authors

**Conference name**:  ICLR 2024

**URL**:https://openreview.net/attachment?id=aIok3ZD9to&name=pdf

**Your summary**:
This paper mainly tries the address the issues of Large language models and it's environmental impacts. They talk about the development of LLMCarbon which is a carbon footprint projection model for large language models (LLMs). They propose a solution for estimating LLMs carbon footprints across different stages in their lifecycle.

Due to large training, inference and storage requiremnts, LLMs turn out to have a significant impact in the carbon forrprint. Estimating this footprint level before the development and deployment of the model is crucial. There is an existing tool called mlco2 which predicts the carbon footprint of ML models but it has it's limitation when it comes in supporting LLMs. Thus LLMCarbon is proposed which is an end to end carbon footprint projection model. They have taken note of various factors like diverse hardware, architecture, operational phases, data center efficiency etc in the LLMCarbon in order to provide accurate results.Overall, LLMCarbon helps significantly in understanding and mitigating the impact of LLms in the environment. Thus, this helps stakeholders to take informed decisions regarding the development and allocation of a particular LLM development.

The LLMCarbon is also Validated in various phases. LLMCarbon's operational footprint predictions are validated against five LLMs developed by Google, OpenAI, and Meta during their training phases, as well as against an LLM called Noor during its storage phase.For the training phase validation, LLMCarbon's predictions are compared to actual operational footprint data for LLMs such as T5, GPT-3, etc. LLMCarbon provides accurate predictions for both operational and embodied carbon footprints across various phases of LLM development and deployment.

The paper also explores various aspects of LLMCarbon including detailed explanation of the different models employes within the LLMCarbon. LLMCarbon utilizes a series of models including the parameter model, neural scaling law, FLOP model, hardware efficiency model, operational carbon model, and embodied carbon model to compute the carbon footprint. I had difficulty in understanding this part of the paper.

## AI Disclosure

*   Did you use any AI assistance to complete this homework? If so, please also specify what AI you used.
    * Yes. ChatGPT


---
*(only complete the below questions if you answered yes above)*

*   If you used a large language model to assist you, please paste *all* of the prompts that you used below. Add a separate bullet for each prompt, and specify which problem is associated with which prompt.

    * (1.3) for any prefix  w1,w2,…,wi−1 , retrieve all occurrences of that prefix in some huge text corpus (such as the Common Crawl) and keep count of the word  wi  that follows each occurrence. I can then use this to estimate the conditional probability  P(wi|w1,w2,…,wi−1)  for any prefix. Explain why this method is completely impractical!
    * (2.2) what is the python function to remove punctuations from a given paragraph
    * (2.3) Why are stopwords not used in NLP during computation?
    * (3.3) What does this error mean?
    --> 18 projected_x = torch.mm(W, x)
RuntimeError: mat2 must be a matrix

*   **Free response**: For each problem for which you used assistance, describe your overall experience with the AI. How helpful was it? Did it just directly give you a good answer, or did you have to edit it? Was its output ever obviously wrong or irrelevant? Did you use it to get the answer or check your own answer?
    * The AI gave me direct good answers for 1.2, 2.2 and 2.3 but I had to edit and play around to understand my 3.3 error. I used it to get the answer for 1.3, 2.2 and 3.3 whereas I used it for 2.3 to check if my answer was correct.
"""